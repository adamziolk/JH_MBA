png("my.first.plot.png",width=480,height=360)
dev.off(3)
dev.list()
dev.set(2)
# setwd()
getwd()
# boxplot, (try larger size)
x=rpois(lambda=10,50)
boxplot(x)
par(mfrow=c(1,2))
boxplot(x)
boxplot(log(x))
par(mfrow=c(1,1))
# Read Data
# Use read.table() or read.csv() to read data into R
Auto=read.csv("Auto.csv",header=T,na.strings="?")
# read data from the Internet
theURL <- "http://www.jaredlander.com/data/Tomato%20First.csv"
tomato <- read.table(file=theURL,header=TRUE, sep=",")
head(tomato)
# Probability Distributions
pnorm(2,mean=5,sd=10)
dnorm(2,mean=5,sd=10)
qnorm(.38,mean=5,sd=10)
z=rnorm(mean=5,sd=100,n=10)
# Example: Uniform Distribution
dunif(x=8,min=5,max=15)
punif(10,min=5,max=15)
qunif(.8,min=5,max=15)
runif(10,min=5,max=15)
# Example: Normal Distribution
set.seed(5)
rnorm(3,mean=10,sd=20)
rnorm(3,mean=10,sd=20)
set.seed(5)
rnorm(3,mean=10,sd=20)
# Sample Function
sample(1:40,5)
sample(c("H","T"),10,replace=T)
sample(c("success","failure"),10,replace=T,prob=c(0.8,0.2))
# Probability
x<-seq(-4,4,0.01)
plot(x,dnorm(x),type="l")
x<-0:50
plot(x,dbinom(x,size=50,prob=.33),type="h")
# Define a Function
f<-function(x){3*x^(-4)}
f(2)
# Verify whether a function is a well-defined density function
integrate(f,1,Inf)
# Simulation: Generate Random Variables Following Any Distribution
set.seed(13)
U=runif(1000)
X=(1-U)^(-1/3)
View(tomato)
setwd("~/Documents/JH_MBA/BU.510.650.33_Data_Analytics/Module4")
getwd()
library(ISLR)
attach(Smarket)
head(Smarket)
tail(Smarket)
summary(Smarket)
cor(Smarket)
cor(Smarket[,-9])
#### plots
plot(Smarket)
plot(Smarket$Today~Smarket$Lag1)
plot(Smarket$Lag1,Smarket$Today)
hist(Smarket$Today)
hist(Smarket$Today,breaks=5)
hist(Today,breaks=5)
hist(Today,breaks=10)
hist(Today,breaks=100)
#### logistic regression
glm.fit=glm(Direction~Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
family=binomial,data=Smarket)
summary(glm.fit)
predict(glm.fit,type="response")->glm.probs
head(glm.probs)
options("digits"=2)
glm.probs[1:6]
contrasts(Direction)
glm.pred=rep("Down",1250)
glm.pred[glm.probs>.5]="Up"
head(glm.pred)
head(Direction)
table(glm.pred,Direction)
(145+507)/1250
mean(glm.pred==Direction)
levels(Smarket$Year)
class(Smarket$Year)
train=(Year<2005)
Smarket.2005=Smarket[!train,]
dim(Smarket.2005)
Direction.2005=Direction[!train]
head(Direction.2005)
length(Direction.2005)
glm.fit=glm(Direction~Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
family=binomial,data=Smarket,subset=train)
glm.probs=predict(glm.fit,newdata=Smarket.2005,type="response")
glm.pred[glm.probs>.5]="Up"
glm.pred=rep("Down",252)
glm.pred[glm.probs>.5]="Up"
mean(glm.pred==Direction.2005)
# improve prediction accuracy
glm.fit=glm(Direction ~ Lag1+Lag2, data=Smarket, family=binomial,subset=train)
glm.probs=predict(glm.fit,Smarket.2005,type="response")
glm.pred=rep("Down",252)
glm.pred[glm.probs>.5]="Up"
table(glm.pred,Direction.2005)
mean(glm.pred==Direction.2005)
############## Auto Data Set
Auto=read.csv("Auto.csv",header=T,na.strings="?")
############## Auto Data Set
Auto=read.csv("Auto.csv",header=T,na.strings="?")
Auto2=na.omit(Auto) # remove missing values
mpg.median=median(Auto2$mpg)
mpg01 <- ifelse(Auto2$mpg > mpg.median, 1, 0)
Auto3=data.frame(mpg01,Auto2[,-1]) # create a new data frame
head(Auto3)
# explore the data w.r.t. mgp01
boxplot(mpg01~cylinders, data=Auto3)
plot(mpg01~weight, data=Auto3)
plot(mpg01~horsepower, data=Auto3)
plot(mpg01~acceleration, data=Auto3)
train= seq(1,nrow(Auto3)/2) # create indeces for training data set
train
test= seq(nrow(Auto3)/2+1, nrow(Auto3))
test
# perform logistic regression
logit.auto=glm(mpg01~ cylinders+weight+horsepower +acceleration, family="binomial",data=Auto3)
summary(logit.auto)
# p-value for acceleration is not significant, re-perform logistic regression without it
logit.auto=glm(mpg01~ cylinders+weight+horsepower, family="binomial",data=Auto3)
summary(logit.auto)
# you may also want to remove cylinders
logit.auto=glm(mpg01~ weight+horsepower, family="binomial",data=Auto3)
summary(logit.auto)
# perform logistic regression with training data, use testing data for accuracy
logit.auto2=glm(mpg01~ weight+horsepower, family="binomial",data=Auto3,subset=train)
auto.probs=predict(logit.auto2,newdata=Auto3[test,],type="response")
auto.pred=rep("Down",length(test))
auto.pred[auto.probs>.5]="Up"
table(auto.pred,Auto3$mpg01[test]) ->test.table
prediction.accuracy <- (test.table[1,1]+test.table[2,2])/sum(test.table)
prediction.accuracy
#################################################
#### multinomial logistic regression
#################################################
library(VGAM) ## VGAM to estimate multinomial logistic regression
library(textir)## to standardize the features
install.packages("VGAM")
library(textir)## to standardize the features
library(MASS)## a library of example datasets
install.packages("textir")
###### k-nearest neighbor
library(class)
getwd()
library(ISLR)
attach(Smarket)
head(Smarket)
tail(Smarket)
summary(Smarket)
cor(Smarket)
cor(Smarket[,-9])
#### plots
plot(Smarket)
plot(Smarket$Today~Smarket$Lag1)
plot(Smarket$Lag1,Smarket$Today)
hist(Smarket$Today)
hist(Smarket$Today,breaks=5)
hist(Today,breaks=5)
hist(Today,breaks=10)
hist(Today,breaks=100)
#### logistic regression
glm.fit=glm(Direction~Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
family=binomial,data=Smarket)
summary(glm.fit)
predict(glm.fit,type="response")->glm.probs
head(glm.probs)
options("digits"=2)
glm.probs[1:6]
contrasts(Direction)
glm.pred=rep("Down",1250)
glm.pred[glm.probs>.5]="Up"
head(glm.pred)
head(Direction)
table(glm.pred,Direction)
(145+507)/1250
mean(glm.pred==Direction)
levels(Smarket$Year)
class(Smarket$Year)
train=(Year<2005)
Smarket.2005=Smarket[!train,]
dim(Smarket.2005)
Direction.2005=Direction[!train]
head(Direction.2005)
length(Direction.2005)
glm.fit=glm(Direction~Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
family=binomial,data=Smarket,subset=train)
glm.probs=predict(glm.fit,newdata=Smarket.2005,type="response")
glm.pred[glm.probs>.5]="Up"
glm.pred=rep("Down",252)
glm.pred[glm.probs>.5]="Up"
mean(glm.pred==Direction.2005)
# improve prediction accuracy
glm.fit=glm(Direction ~ Lag1+Lag2, data=Smarket, family=binomial,subset=train)
glm.probs=predict(glm.fit,Smarket.2005,type="response")
glm.pred=rep("Down",252)
glm.pred[glm.probs>.5]="Up"
table(glm.pred,Direction.2005)
mean(glm.pred==Direction.2005)
############## Auto Data Set
Auto=read.csv("Auto.csv",header=T,na.strings="?")
Auto2=na.omit(Auto) # remove missing values
mpg.median=median(Auto2$mpg)
mpg01 <- ifelse(Auto2$mpg > mpg.median, 1, 0)
Auto3=data.frame(mpg01,Auto2[,-1]) # create a new data frame
head(Auto3)
# explore the data w.r.t. mgp01
boxplot(mpg01~cylinders, data=Auto3)
plot(mpg01~weight, data=Auto3)
plot(mpg01~horsepower, data=Auto3)
plot(mpg01~acceleration, data=Auto3)
train= seq(1,nrow(Auto3)/2) # create indeces for training data set
train
test= seq(nrow(Auto3)/2+1, nrow(Auto3))
test
# perform logistic regression
logit.auto=glm(mpg01~ cylinders+weight+horsepower +acceleration, family="binomial",data=Auto3)
summary(logit.auto)
# p-value for acceleration is not significant, re-perform logistic regression without it
logit.auto=glm(mpg01~ cylinders+weight+horsepower, family="binomial",data=Auto3)
summary(logit.auto)
# you may also want to remove cylinders
logit.auto=glm(mpg01~ weight+horsepower, family="binomial",data=Auto3)
summary(logit.auto)
# perform logistic regression with training data, use testing data for accuracy
logit.auto2=glm(mpg01~ weight+horsepower, family="binomial",data=Auto3,subset=train)
auto.probs=predict(logit.auto2,newdata=Auto3[test,],type="response")
auto.pred=rep("Down",length(test))
auto.pred[auto.probs>.5]="Up"
table(auto.pred,Auto3$mpg01[test]) ->test.table
prediction.accuracy <- (test.table[1,1]+test.table[2,2])/sum(test.table)
prediction.accuracy
#################################################
#### multinomial logistic regression
#################################################
library(VGAM) ## VGAM to estimate multinomial logistic regression
library(textir)## to standardize the features
library(MASS)## a library of example datasets
data(fgl)## loads the data into R; see help(fgl)
fgl[1:3,]
gg <- vglm(type ~ Na+Mg+Al,multinomial,data=fgl)
summary(gg)
dWinF=fgl$type=="WinF"
dWinNF=fgl$type=="WinNF"
dVeh=fgl$type=="Veh"
dCon=fgl$type=="Con"
dTable=fgl$type=="Tabl"
dHead=fgl$type=="Head"
yy1=c(fitted(gg)[dWinF,1],fitted(gg)[dWinNF,2],fitted(gg)[dVeh,3], fitted(gg)[dCon,4],fitted(gg)[dTable,5],fitted(gg)[dHead,6])
xx1=c(fgl$type[dWinF],fgl$type[dWinNF],fgl$type[dVeh], fgl$type[dCon], fgl$type[dTable], fgl$type[dHead])
boxplot(yy1~xx1,ylim=c(0,1),xlab="1=WinF,2=WinNF,3=Veh, 4=Con,5=Table,6=Head")
# more boxplots
par(mfrow=c(2,3))
plot(RI ~ type, data=fgl, col=c(grey(.2),2:6))
plot(Al ~ type, data=fgl, col=c(grey(.2),2:6))
plot(Na ~ type, data=fgl, col=c(grey(.2),2:6))
plot(Mg ~ type, data=fgl, col=c(grey(.2),2:6))
plot(Ba ~ type, data=fgl, col=c(grey(.2),2:6))
plot(Si ~ type, data=fgl, col=c(grey(.2),2:6))
par(mfrow=c(1,1))
#####################################################
#### K-Nearest Neighbors: Example
#####################################################
library(ISLR)
attach(Smarket)
data(Smarket)
train=(Year<2005)
Smarket.2005=Smarket[!train,]
dim(Smarket.2005)
###### k-nearest neighbor
library(class)
train.X=cbind(Lag1,Lag2)[train,]
test.X=cbind(Lag1,Lag2)[!train,]
train.Direction=Direction[train]
Direction.2005=Direction[!train]
set.seed(1)
knn.pred=knn(train.X,test.X,train.Direction,k=1)
table(knn.pred,Direction.2005)
mean(knn.pred==Direction.2005)
knn.pred=knn(train.X,test.X,train.Direction,k=3)
mean(knn.pred==Direction.2005)
plot(Smarket$Lag1,Smarket$Today)
#### plots
plot(Smarket)
library(ISLR)
attach(Smarket)
head(Smarket)
tail(Smarket)
summary(Smarket)
cor(Smarket)
cor(Smarket[,-9])
#### plots
plot(Smarket)
plot(Smarket$Today~Smarket$Lag1)
plot(Smarket$Lag1,Smarket$Today)
plot(Smarket$Lag1,Smarket$Today)
getwd()
library(ISLR)
attach(Smarket)
train=(Year<2005)
Smarket.2005=Smarket[!train,]
dim(Smarket.2005)
Direction.2005=Direction[!train]
head(Direction.2005)
length(Direction.2005)
glm.fit=glm(Direction~Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
family=binomial,data=Smarket,subset=train)
glm.probs=predict(glm.fit,newdata=Smarket.2005,type="response")
glm.pred[glm.probs>.5]="Up"
glm.pred=rep("Down",1250)
glm.pred[glm.probs>.5]="Up"
library(ISLR)
attach(Smarket)
train=(Year<2005)
Smarket.2005=Smarket[!train,]
glm.fit=glm(Direction~Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
family=binomial,data=Smarket,subset=train)
glm.probs=predict(glm.fit,newdata=Smarket.2005,type="response")
glm.pred=rep("Down",252)
glm.pred[glm.probs>.5]="Up"
# Q2
library(ISLR)
attach(Smarket)
train=(Year<2005)
Smarket.2005=Smarket[!train,]
Direction.2005=Direction[!train]
glm.fit=glm(Direction~Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
family=binomial,data=Smarket,subset=train)
glm.probs=predict(glm.fit,newdata=Smarket.2005,type="response")
glm.pred=rep("Down",252)
glm.pred[glm.probs>.5]="Up"
mean(glm.pred==Direction.2005)
glm.fit2=glm(Direction~Lag1 + Lag2,
family=binomial,data=Smarket,subset=train)
glm.probs2=predict(glm.fit2,newdata=Smarket.2005,type="response")
glm.pred2=rep("Down",252)
glm.pred2[glm.probs2>.5]="Up"
mean(glm.pred2==Direction.2005)
#######################################################
# We can analyze th Smarket data in many ways.
# Here we present 2 examples
require(ISLR)
Smarket=Smarket
names(Smarket)
head(Smarket)
attach(Smarket)
#####################################################
# Logistic Model
logistic.fit=glm(Direction~Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, family = binomial, data=Smarket)
summary(logistic.fit)
pchisq(1731.2-1727.6, 1249-1243)
names(logistic.fit)
pchisq(logistic.fit$null.deviance-logistic.fit$deviance, logistic.fit$df.null-logistic.fit$df.residual)
train.data=subset(Smarket, Year<2005)
test.data=subset(Smarket, Year==2005)
train.mod=glm(Direction~Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, family = binomial, data=train.data)
test.probs=predict(train.mod, test.data, type="response")
head(test.probs)
require(psych)
describe(test.probs)
pred.directions.test=ifelse(test.probs<=0.5, "Down", "Up")
head(pred.directions.test)
mean(pred.directions.test==test.data$Direction)
table(pred.directions.test, test.data$Direction)
######################################################
# KNN Approach
require(class)
knn1.fit=knn(train.data[,c(2:7)], test.data[,c(2:7)], train.data$Direction, 1)
head(knn1.fit)
OneNN.error = 1 - mean(knn1.fit==test.data$Direction);OneNN.error
# Consider higher values of k
knn3.fit=knn(train.data[,c(2:7)], test.data[,c(2:7)], train.data$Direction, 3)
ThreeNN.error = 1 - mean(knn3.fit==test.data$Direction); ThreeNN.error
knn5.fit = knn(train.data[,c(2:7)], test.data[,c(2:7)], train.data$Direction, 5)
FiveNN.error = 1 - mean(knn5.fit==test.data$Direction); FiveNN.error
knn7.fit = knn(train.data[,c(2:7)], test.data[,c(2:7)], train.data$Direction, 7)
SevenNN.error = 1 - mean(knn7.fit==test.data$Direction); SevenNN.error
plot(c(1,3,5,7), c(OneNN.error, ThreeNN.error, FiveNN.error, SevenNN.error), type="b")
######################################################
# K-means clusters with simulated data
set.seed(1)
x.coords=(rnorm(50))
y.coords=(rnorm(50))
group.num=c(1,2)
points=data.frame(cbind(group.num, x.coords, y.coords))
View(points)
plot(points$x.coords, points$y.coords, col=group.num, pch=16)
points$x.coords[c(1:25)] = points$x.coords[c(1:25)]+3
points$y.coords[c(1:25)] = points$y.coords[c(1:25)]-4
plot(points$x.coords, points$y.coords, col=points$group.num, pch=16)
points$group.num[c(1:25)]=1
points$group.num[c(26:50)]=2
plot(points$x.coords, points$y.coords, col=points$group.num, pch=16)
km.out=kmeans(points, 2, nstart=20)
plot(points$x.coord, points$y.coord, col=km.out$cluster, pch=16, main="New Plot")
names(km.out)
km.out$cluster
km.out$totss
km.out$withinss
km.out$tot.withinss
km.out$betweenss
right = mean(points$group.num==km.out$cluster); c(right, 1-right)
# Consider 3 clusters
km3.out=kmeans(points, 3, nstart=20)
plot(points$x.coord, points$y.coord, col=km3.out$cluster, pch=16)
c(km.out$totss, km.out$tot.withinss, km.out$betweenss)
c(km3.out$totss, km3.out$tot.withinss, km3.out$betweenss)
##################################################
# Another clustering example: Red and White Wines
##################################################
wine <- read.csv("wine.csv")
require(psych)
describe(wine)
summary(wine$color)
# Question: can we tell the difference between Red wines and White wines without looking at the color?
# Fit wines into 2 clusters based on quantitative differences and see if clusters match colors.
wine.data=wine[,1:11]
km2.out=kmeans(wine.data, 2, nstart = 10)
?knn
data = read.csv("VanderbiltSched.csv")
View(data)
View(data)
# Q1
data$T...7
# Q1
plot(data$T...7, data$Actual)
# Q2
lm(Actual!T...7, data=data)
# Q2
lm(Actual~T...7, data=data)
# Q2
linreg = lm(Actual~T...7, data=data)
abline(linreg)
View(data)
# Q3
data$Actual-data$T...1
# Q3
q3 = data.frame(c(data$DOW, data$Actual-data$T...1))
View(q3)
# Q3
q3 = data.frame(data$DOW, data$Actual-data$T...1)
View(q3)
View(q3)
aggregate(q3[, 2], list(q3$data.DOW), mean)
View(data)
# Q7
q7 = data[, c(1, 2, 18, 19)]
View(q7)
45*5
q7_train = q7[:225, ]
q7_train = q7[0:225, ]
q7_test = q7[225:, ]
View(q7_train)
summary(q7_train)
View(q7)
q7_test = q7[225:241, ]
225+17
View(q7_train)
View(q7_test)
q7_test = q7[226:241, ]
glm.fit=glm(Actual ~ T...7, data=q7_train, family=binomial,subset=train)
glm.fit=glm(Actual ~ T...1, data=q7_train, family=binomial,subset=train)
glm.fit=glm(Actual ~ T...1, data=q7_train, family=binomial,subset=train)
glm.fit=glm(Actual ~ T...1, data=q7_train, family=binomial)
View(q7_train)
q7_lm = lm(Actual~T...7, data=q7_train)
q7_lm = lm(Actual~T...1, data=q7_train)
# Q9
predict(q7_lm, q7_test$T...1)
#predict(lm.fit,data.frame(lstat=(c(5,10,15))),interval="confidence")
q7_test$T...1
# Q9
q9_data = data.frame(T...1 = q7_test$T...1)
View(q9_data)
predict(q7_lm, q9_data)
q9_pred = predict(q7_lm, q9_data)
q9_actual = data.frame(Actual = q7_test$Actual)
mean((q9_pred - q9_actual)^2)
View(q9_actual)
q9_pred = predict(q7_lm, q9_data)
View(q9_actual)
q9_pred = data.frame(predict(q7_lm, q9_data))
View(q9_pred)
mean((q9_pred - q9_actual)^2)
q9_pred
q9_actual
mean((q9_pred - q9_actual)^2)
q9_pred - q9_actual
(q9_pred - q9_actual)^2
mean((q9_pred - q9_actual)^2)
wtf = (q9_pred - q9_actual)^2
mean(wtf)
wtf
wtf.summary()
summary(wtf)
View(wtf)
sq_diff = (q9_pred - q9_actual)^2
mean(sq_diff)
colMeans(sq_diff)
