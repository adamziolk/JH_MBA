aa
# Create a Matrix
a<-1:5
b<-rnorm(5)
# make a matrix by column binding
c.matrix<-cbind(a,b)
# names of rows and columns
rownames(c.matrix)
colnames(c.matrix)
# Indexing for matrices
c.matrix[4,2]
b
c.matrix[1,]
a
b
c.matrix[,2]
c.matrix[c.matrix>1]
# Matrix Operations
# create a matrix with 2 columns and 3 rows
# filled with random normal values
m.normal=matrix(rnorm(6),nrow=3)
m2=m.normal*10
m2
m2[,2]=m2[,2]+50
summary(m2)
# Matrices Versus Data Frames
x=1:10
y=rnorm(10)
mat<-cbind(x,y)
class(mat[,1])
z=paste0('a',1:10)
tab<-cbind(x,y,z)
class(tab)
mode(tab[,1])
head(tab,4)
tab<-data.frame(x,y,z)
class(tab)
head(tab)
mode(tab[,1])
rownames(tab)
rownames(tab)<-paste0("row",1:10)
rownames(tab)
# Data frame columns can be refered to by name using the "dollar sign" operator $
tab$x
attach(tab)
x
# Column names can be set, which can be useful for referring to data later
colnames(tab)
colnames(tab)<-c('a','b','c')
colnames(tab)
colnames(tab)<-paste0('col',1:3)
colnames(tab)
# A list is a collection of objects that may be the same or different types.
# A data frame is a list of matched column vectors.
# Create a list
x=list(1,"y",c(2,4,6))
x
length(x)
class(x)
x[[2]]
is.list(tab)
tab[[2]]
names(tab)
# Basic Plot Functions
x=rnorm(50)
y=seq(from=0,to=100,length.out=50)
plot(x,y,xlab='x normal random',ylab='y sequence', main='plot test', pch=5,col=4)
# plot(x)
lines(x,y)
points(x,y)
# Save a png image to a file
png("my.first.plot.png",width=480,height=360)
dev.off(3)
dev.list()
dev.set(2)
# setwd()
getwd()
# boxplot, (try larger size)
x=rpois(lambda=10,50)
boxplot(x)
par(mfrow=c(1,2))
boxplot(x)
boxplot(log(x))
par(mfrow=c(1,1))
# Read Data
# Use read.table() or read.csv() to read data into R
Auto=read.csv("Auto.csv",header=T,na.strings="?")
# read data from the Internet
theURL <- "http://www.jaredlander.com/data/Tomato%20First.csv"
tomato <- read.table(file=theURL,header=TRUE, sep=",")
head(tomato)
# Probability Distributions
pnorm(2,mean=5,sd=10)
dnorm(2,mean=5,sd=10)
qnorm(.38,mean=5,sd=10)
z=rnorm(mean=5,sd=100,n=10)
# Example: Uniform Distribution
dunif(x=8,min=5,max=15)
punif(10,min=5,max=15)
qunif(.8,min=5,max=15)
runif(10,min=5,max=15)
# Example: Normal Distribution
set.seed(5)
rnorm(3,mean=10,sd=20)
rnorm(3,mean=10,sd=20)
set.seed(5)
rnorm(3,mean=10,sd=20)
# Sample Function
sample(1:40,5)
sample(c("H","T"),10,replace=T)
sample(c("success","failure"),10,replace=T,prob=c(0.8,0.2))
# Probability
x<-seq(-4,4,0.01)
plot(x,dnorm(x),type="l")
x<-0:50
plot(x,dbinom(x,size=50,prob=.33),type="h")
# Define a Function
f<-function(x){3*x^(-4)}
f(2)
# Verify whether a function is a well-defined density function
integrate(f,1,Inf)
# Simulation: Generate Random Variables Following Any Distribution
set.seed(13)
U=runif(1000)
X=(1-U)^(-1/3)
View(tomato)
install.packages("tree")
########################################
#### Decision Tree
########################################
library(tree)
library(ISLR)
attach(Carseats)
# data(Carseats)
High=ifelse(Sales <= 8, "No", "Yes")
Carseats2 = data.frame(Carseats, High)
tree.carseats = tree(High~.-Sales, Carseats2)
summary(tree.carseats)
#### plot() display tree structure
plot(tree.carseats)
#### text() display the node labels
## pretty=0 includes the category names
text(tree.carseats,pretty=0)
tree.carseats
#### calcuate training error and testing error
set.seed(2)
train = sample(1:nrow(Carseats2), 200)
Carseats2.test = Carseats2[-train,]
High.test = High[-train]
tree.carseats = tree(High~.-Sales, Carseats2, subset=train)
tree.pred = predict(tree.carseats, Carseats2.test, type="class")
table(tree.pred, High.test)
(86+57)/200
#### prune the tree
set.seed(3)
cv.carseats = cv.tree(tree.carseats, FUN=prune.misclass)
names(cv.carseats)
cv.carseats
par(mfrow=c(1,2))
plot(cv.carseats$size,cv.carseats$dev,type="b")
plot(cv.carseats$k,cv.carseats$dev,type="b")
prune.carseats = prune.misclass(tree.carseats,best=9)
par(mfrow=c(1,1))
plot(prune.carseats)
text(prune.carseats,pretty=0)
#### Fitting regression trees
library(MASS)
set.seed(1)
train = sample(1:nrow(Boston), nrow(Boston)/2) # split into to equal subsets
tree.boston = tree(medv~., data=Boston, subset=train)
summary(tree.boston)
plot(tree.boston)
text(tree.boston,pretty=0)
## use function cv.tree() function to see whether prunning the tree will improve performance
cv.boston = cv.tree(tree.boston)
names(cv.boston)
summary(cv.boston)
plot(cv.boston$size, cv.boston$dev, type='b')
cv.boston$dev
## prune tree
prune.boston = prune.tree(tree.boston, best=5)
plot(prune.boston)
text(prune.boston, pretty=0)
## make prediction
yhat = predict(tree.boston, newdata=Boston[-train,])
#attach(Boston)
boston.test= Boston[-train,"medv"]
plot(yhat, boston.test)
abline(0,1)
mean((yhat - boston.test)^2)
#### in-class exercise
lm.boston = lm(medv~lstat + rm + dis, data=Boston, subset=train)
pred.boston = predict(lm.boston, newdata= Boston[-train,])
mean((pred.boston- Boston$medv[-train])^2)
## PCA sample code
states=row.names(USArrests)
states
names(USArrests)
tail(USArrests)
apply(USArrests,2,mean)
apply(USArrests,2,var)
pr.out=prcomp(USArrests,scale=T)
names(pr.out)
pr.out$center
pr.out$scale
pr.out$rotation
summary(pr.out)
dim(pr.out$x)
##### plot the first two principal components
biplot(pr.out, scale=0)
?biplot
#### mirror image
pr.out$rotation = -pr.out$rotation
pr.out$x = -pr.out$x
biplot(pr.out, scale=0)
#### The prcomp() function also outputs the standard deviation of each principal component.
pr.out$sdev
pr.var=pr.out$sdev^2
pr.var
pve=pr.var/sum(pr.var)
pve
##### plots
par(mfrow=c(1,2))
plot(pve , xlab=" Principal Component ", ylab=" Proportion of Variance Explained ",
ylim=c(0,1) ,type="b")
plot(cumsum (pve ), xlab=" Principal Component ", ylab ="Cumulative Proportion of Variance Explained ",
ylim=c(0,1), type="b")
par(mfrow=c(1,1))
##### in-class exercise: Boston data set
library(MASS)
attach(Boston)
boston.pca=prcomp(Boston,scale=T)
summary(boston.pca)
library(pls)
library(ISLR)
pcr.fit=pcr(Salary~.,data=Hitters,scale=T,validation="CV")
install.packages("pls")
## PCA sample code
states=row.names(USArrests)
states
names(USArrests)
tail(USArrests)
apply(USArrests,2,mean)
apply(USArrests,2,var)
pr.out=prcomp(USArrests,scale=T)
names(pr.out)
pr.out$center
pr.out$scale
pr.out$rotation
summary(pr.out)
dim(pr.out$x)
##### plot the first two principal components
biplot(pr.out, scale=0)
?biplot
#### mirror image
pr.out$rotation = -pr.out$rotation
pr.out$x = -pr.out$x
biplot(pr.out, scale=0)
#### The prcomp() function also outputs the standard deviation of each principal component.
pr.out$sdev
pr.var=pr.out$sdev^2
pr.var
pve=pr.var/sum(pr.var)
pve
##### plots
par(mfrow=c(1,2))
plot(pve , xlab=" Principal Component ", ylab=" Proportion of Variance Explained ",
ylim=c(0,1) ,type="b")
plot(cumsum (pve ), xlab=" Principal Component ", ylab ="Cumulative Proportion of Variance Explained ",
ylim=c(0,1), type="b")
par(mfrow=c(1,1))
##### in-class exercise: Boston data set
library(MASS)
attach(Boston)
boston.pca=prcomp(Boston,scale=T)
summary(boston.pca)
library(pls)
library(ISLR)
pcr.fit=pcr(Salary~.,data=Hitters,scale=T,validation="CV")
summary(pcr.fit)
# plot cross-validation scores
validationplot(pcr.fit,val.type="MSEP")
pcr.fit2=pcr(Salary~.,data=Hitters,scale=T,validation="CV",ncomp=7)
summary(pcr.fit2)
# regression on subset, make predictions
set.seed(1)
Hitters=na.omit(Hitters)
x = model.matrix(Salary~., Hitters)[,-1]
y=Hitters$Salary
train = sample(1:nrow(x), nrow(x)/2)
test=(-train)
y.test=y[test]
pcr.fit = pcr(Salary~., data= Hitters, subset=train, scale=T, validation = "CV")
pcr.pred = predict(pcr.fit, x[test,], ncomp=7)
# calculate prediction errors
mean((pcr.pred - y.test)^2)
#### Partial Least Squares
set.seed(1)
Hitters=na.omit(Hitters)
x = model.matrix(Salary~., Hitters)[,-1]
y=Hitters$Salary
train = sample(1:nrow(x), nrow(x)/2)
test=(-train)
y.test=y[test]
pls.fit = plsr(Salary ~., data=Hitters, subset=train, scale=TRUE, validation ="CV")
summary(pls.fit)
validationplot(pls.fit,val.type="MSEP")
## evaluate test set MSE
pls.pred = predict(pls.fit,x[test,],ncomp=2)
mean((pls.pred - y.test)^2)
# perform PLS using M=2
pls.fit = plsr(Salary ~., data=Hitters, scale=TRUE, ncomp=2)
summary(pls.fit)
## PCA sample code
states=row.names(USArrests)
states
names(USArrests)
tail(USArrests)
apply(USArrests,2,mean)
apply(USArrests,2,var)
pr.out=prcomp(USArrests,scale=T)
names(pr.out)
pr.out$center
pr.out$scale
pr.out$rotation
summary(pr.out)
dim(pr.out$x)
##### plot the first two principal components
biplot(pr.out, scale=0)
?biplot
#### mirror image
pr.out$rotation = -pr.out$rotation
pr.out$x = -pr.out$x
biplot(pr.out, scale=0)
#### The prcomp() function also outputs the standard deviation of each principal component.
pr.out$sdev
pr.var=pr.out$sdev^2
pr.var
pve=pr.var/sum(pr.var)
pve
##### plots
par(mfrow=c(1,2))
plot(pve , xlab=" Principal Component ", ylab=" Proportion of Variance Explained ",
ylim=c(0,1) ,type="b")
plot(cumsum (pve ), xlab=" Principal Component ", ylab ="Cumulative Proportion of Variance Explained ",
ylim=c(0,1), type="b")
par(mfrow=c(1,1))
##### in-class exercise: Boston data set
library(MASS)
attach(Boston)
boston.pca=prcomp(Boston,scale=T)
summary(boston.pca)
library(pls)
library(ISLR)
pcr.fit=pcr(Salary~.,data=Hitters,scale=T,validation="CV")
summary(pcr.fit)
# plot cross-validation scores
validationplot(pcr.fit,val.type="MSEP")
pcr.fit2=pcr(Salary~.,data=Hitters,scale=T,validation="CV",ncomp=7)
summary(pcr.fit2)
# regression on subset, make predictions
set.seed(1)
Hitters=na.omit(Hitters)
x = model.matrix(Salary~., Hitters)[,-1]
y=Hitters$Salary
train = sample(1:nrow(x), nrow(x)/2)
test=(-train)
y.test=y[test]
pcr.fit = pcr(Salary~., data= Hitters, subset=train, scale=T, validation = "CV")
pcr.pred = predict(pcr.fit, x[test,], ncomp=7)
# calculate prediction errors
mean((pcr.pred - y.test)^2)
#### Partial Least Squares
set.seed(1)
Hitters=na.omit(Hitters)
x = model.matrix(Salary~., Hitters)[,-1]
y=Hitters$Salary
train = sample(1:nrow(x), nrow(x)/2)
test=(-train)
y.test=y[test]
pls.fit = plsr(Salary ~., data=Hitters, subset=train, scale=TRUE, validation ="CV")
summary(pls.fit)
validationplot(pls.fit,val.type="MSEP")
## evaluate test set MSE
pls.pred = predict(pls.fit,x[test,],ncomp=2)
mean((pls.pred - y.test)^2)
# perform PLS using M=2
pls.fit = plsr(Salary ~., data=Hitters, scale=TRUE, ncomp=2)
summary(pls.fit)
setwd("~/Documents/JH_MBA/BU.510.650.33_Data_Analytics/Project")
require(data.table)
data = as.data.frame(fread("marketing_campaign.csv"))
# Remove missing data
sum(is.na(data))
data = data[complete.cases(data), ]
sum(is.na(data))
summary(data) # Provides Quantilies and stats about each variable
sum(duplicated(data$ID)) # Each record is a unique customer
# What is the average household income of individuals purchasing wine via the company’s website versus directly in stores?
web_customers = data[data$NumWebPurchases > 0 & data$NumStorePurchases == 0, ] # 5 Records
View(web_customers)
NumWebPurchases
store_customers =  data[data$NumWebPurchases == 0 & data$NumStorePurchases > 0, ] # 39 Records
# Insufficient data to answer this question
mean(web_customers)
# Insufficient data to answer this question
mean(web_customers$Income)
mean(store_customers$Income)
# Is there a correlation between the amount spent on wine versus other products?
# This can be answered by regressing wine purchases against other purchases and viewing the predictive power
lm_wine = lm(data$MntWines ~
data$MntFishProducts+
data$MntFruits+
data$MntGoldProds+
data$MntMeatProducts+
data$MntSweetProducts)
summary(lm_wine)
View(data)
# Is there a correlation between the number of kids and teens in a household and the number of purchases made with a discount?
lm_discounts = lm(data$NumDealsPurchases ~ data$Kidhome+data$Teenhome)
summary(lm_discounts)
# How many people visit the company’s website and how many people purchase through the website?
web_visitors = data[data$NumWebVisitsMonth > 0, ] # 2229 visitors
web_purchasers = data[data$NumWebPurchases > 0, ] # 2191 purchasers
# Is there a correlation between the number of people who accepted the discount offer in the first campaign versus the number of people who accepted the offer in a later campaign?
lm_acceptance = lm(data$AcceptedCmp1 ~
data$AcceptedCmp2+
data$AcceptedCmp3+
data$AcceptedCmp4+
data$AcceptedCmp5)
summary(lm_acceptance)
### Feature Engineering ###
env_data = data[, c("MntWines", "Education", "Kidhome", "Teenhome", "Income", "Year_Birth", "Marital_Status", "Dt_Customer")]
# Convert Marital Status to Factor and clean up
unique(env_data$Marital_Status)
table(env_data$Marital_Status)
Single
table(env_data$Marital_Status)
env_data[env_data$Marital_Status == "Alone", "Marital_Status"] = "Single"
env_data[env_data$Marital_Status == "Absurd" |
env_data$Marital_Status == "YOLO" |
env_data$Marital_Status == "Widow", "Marital_Status"] = "Other"
table(env_data$Marital_Status)
env_data$Marital_Status = factor(env_data$Marital_Status)
is.factor(env_data$Marital_Status)
# Calculate Age
require(stringr)
# Take the last 4 digits (i.e. Year) from transaction date
txn_year = strtoi(str_sub(env_data$Dt_Customer, -4, -1))
# Subtract the birth year to make the Age column
env_data$Age = txn_year - data$Year_Birth
View(env_data)
# Clean up extra columns
env_data$Year_Birth = NULL
env_data$Dt_Customer = NULL
# Can we predict Wine Sales based on Family Environment
lm_wine_env = lm(env_data$MntWines ~
env_data$Education+
env_data$Kidhome+
env_data$Income+
env_data$Teenhome+
env_data$Age+
env_data$Marital_Status)
summary(lm_wine_env)
# Can we predict who will complain
comp_data = env_data
Complain
comp_data$Complain = data$Complain
View(comp_data)
comp_data$MntWines = NULL
set.seed(0)
ind <- sample(2, nrow(comp_data), replace = FALSE, prob = c(0.8, 0.2))
ind <- sample(2, nrow(comp_data), replace = TRUE, prob = c(0.8, 0.2))
comp_data
comp_train <- comp_data[ind==1,]
comp_test <- comp_data[ind==2,]
View(comp_train)
logreg_comp = glm(Complain~., family=binomial, data=comp_train)
comp_probs = predict(logreg_comp, newdata=comp_test, type="response")
plot(comp_probs)
# Balance the data
require(ROSE)
comp_balance <- ovun.sample(Complain~., data = comp_train, method = "over")$data
View(comp_balance)
logreg_comp2 = glm(Complain~., family=binomial, data=comp_balance)
comp_probs2 = predict(logreg_comp2, newdata=comp_test, type="response")
plot(comp_probs2) # Much Better
preds = rep(0, nrow(comp_test))
preds[comp_probs2 > 0.5] = 1
mean(preds==comp_test$Complain) # We can predict who will complain about 63% of the time
mean(preds==comp_test$Complain) # We can predict who will complain about 63% of the time
preds_original = rep(0, nrow(comp_test))
preds_original[comp_probs > 0.5] = 1
mean(preds_original==comp_test$Complain)
# Compare with KNN Results
require(fastDummies)
comp_dummies = dummy_cols(comp_data)
View(comp_dummies)
comp_dummies$Education = NULL
comp_dummies$Marital_Status = NULL
dummy_train <- comp_dummies[ind==1,]
dummy_test <- comp_dummies[ind==2,]
require(class)
knn3 = knn(
dummy_train[, !names(dummy_train) %in% c("Complain")],
dummy_test[, !names(dummy_test) %in% c("Complain")] ,
dummy_train$Complain, 3)
knn3 = knn(
dummy_train[, !names(dummy_train) %in% c("Complain")],
dummy_test[, !names(dummy_test) %in% c("Complain")] ,
dummy_train$Complain, 3)
mean(knn3==dummy_test$Complain) # We can predict who will complain 98% of the time
